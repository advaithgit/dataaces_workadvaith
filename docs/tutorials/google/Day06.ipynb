{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import math\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "def summarize_chunk(chunk):\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summaries = []\n",
        "    for index, row in chunk.iterrows():\n",
        "        article_content = row['content']\n",
        "        parser = PlaintextParser.from_string(article_content, Tokenizer(\"english\"))\n",
        "        summary = summarizer(parser.document, sentences_count=2)\n",
        "        summaries.append(str(summary))\n",
        "    return summaries\n",
        "\n",
        "excel_file_path = \"dataset.xlsx\"\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "num_cores = 4\n",
        "chunk_size = math.ceil(len(df) / num_cores)\n",
        "\n",
        "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "article_summaries = []\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
        "    chunk_summaries = executor.map(summarize_chunk, chunks)\n",
        "    for summaries in chunk_summaries:\n",
        "        article_summaries.extend(summaries)\n",
        "\n",
        "df['summary'] = article_summaries\n",
        "\n",
        "output_excel_file = \"summaries_sumy1.xlsx\"\n",
        "df.to_excel(output_excel_file, index=False)\n",
        "\n",
        "print(\"Summaries saved to\", output_excel_file)"
      ],
      "metadata": {
        "id": "_9S-M-q0Jvds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import math\n",
        "\n",
        "def generate_summary(content):\n",
        "    doc = nlp(content)\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "    summary = \" \".join(sentences[:2])\n",
        "    return summary\n",
        "\n",
        "def summarize_chunk(chunk):\n",
        "    summaries = []\n",
        "    for index, row in chunk.iterrows():\n",
        "        article_content = row['content']\n",
        "        summary = generate_summary(article_content)\n",
        "        summaries.append(summary)\n",
        "    return summaries\n",
        "\n",
        "excel_file_path = \"dataset.xlsx\"\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "\n",
        "num_cores = 4\n",
        "chunk_size = math.ceil(len(df) / num_cores)\n",
        "\n",
        "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "article_summaries = []\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
        "    chunk_summaries = executor.map(summarize_chunk, chunks)\n",
        "    for summaries in chunk_summaries:\n",
        "        article_summaries.extend(summaries)\n",
        "\n",
        "df['summary'] = article_summaries\n",
        "\n",
        "output_excel_file = \"summaries_spacy1.xlsx\"\n",
        "df.to_excel(output_excel_file, index=False)\n",
        "\n",
        "print(\"Summaries saved to\", output_excel_file)\n"
      ],
      "metadata": {
        "id": "qSZoCLYBGjaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import math\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to generate summaries for a chunk of articles\n",
        "def generate_summaries(chunk):\n",
        "    summaries = []\n",
        "    for content in chunk['content']:\n",
        "        # Tokenize the content into sentences\n",
        "        sentences = sent_tokenize(content)\n",
        "\n",
        "        # Remove stopwords and tokenize sentences into words\n",
        "        filtered_sentences = [word_tokenize(sentence) for sentence in sentences if sentence.lower() not in stopwords.words(\"english\")]\n",
        "\n",
        "        # Calculate scores for sentences based on length and position\n",
        "        scores = [len(sentence) for sentence in filtered_sentences]\n",
        "        ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
        "\n",
        "        # Choose the top sentences to form the summary\n",
        "        summary_sentences = [sentence for _, sentence in ranked_sentences[:2]]  # You can adjust the number of sentences\n",
        "\n",
        "        # Combine sentences into summary\n",
        "        summary = \" \".join(summary_sentences)\n",
        "\n",
        "        summaries.append(summary)\n",
        "    return summaries\n",
        "\n",
        "# Load the Excel file using pandas\n",
        "excel_file_path = \"dataset.xlsx\"  # Replace with your actual file path\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "# Calculate the number of chunks based on the available CPU cores\n",
        "num_cores = 4  # Adjust based on your system\n",
        "chunk_size = math.ceil(len(df) / num_cores)\n",
        "\n",
        "# Split the DataFrame into chunks\n",
        "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "# Initialize a list to store summaries\n",
        "article_summaries = []\n",
        "\n",
        "# Process chunks in parallel\n",
        "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
        "    chunk_summaries = executor.map(generate_summaries, chunks)\n",
        "    for summaries in chunk_summaries:\n",
        "        article_summaries.extend(summaries)\n",
        "\n",
        "# Add the summaries to the DataFrame\n",
        "df['summary'] = article_summaries\n",
        "\n",
        "# Save the DataFrame with summaries to a new Excel file\n",
        "output_excel_file = \"summaries_NLTK.xlsx\"\n",
        "df.to_excel(output_excel_file, index=False)\n",
        "\n",
        "print(\"Summaries saved to\", output_excel_file)\n"
      ],
      "metadata": {
        "id": "1vT0QzlgjYaT",
        "outputId": "db60e855-3c87-4761-8040-de6db04ba7cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries saved to summaries_output_NLTK.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "excel_file_path = 'summaries_NLTK.xlsx'\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "summary_column_name = 'summary'\n",
        "snippet_column_name = 'snippet'\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df[snippet_column_name].fillna(''))\n",
        "\n",
        "similarities = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    summary = row[summary_column_name]\n",
        "\n",
        "    try:\n",
        "        summary_vector = tfidf_vectorizer.transform([summary])\n",
        "        snippet_vector = tfidf_matrix[index]\n",
        "        similarity_score = cosine_similarity(summary_vector, snippet_vector)\n",
        "\n",
        "        similarity_percentage = (similarity_score[0][0] + 1) * 50\n",
        "        similarities.append(similarity_percentage)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating similarity for row {index + 1}: {e}\")\n",
        "        similarities.append(0)\n",
        "\n",
        "df['similarity'] = similarities\n",
        "\n",
        "updated_excel_file_path = 'SimilarityNLTK.xlsx'\n",
        "df.to_excel(updated_excel_file_path, index=False)\n",
        "print(\"Similarity percentages added and new Excel file created.\")\n"
      ],
      "metadata": {
        "id": "ZuXpheY8EIFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}