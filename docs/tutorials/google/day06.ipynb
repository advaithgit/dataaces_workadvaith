{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import math\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "def summarize_chunk(chunk):\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summaries = []\n",
        "    for index, row in chunk.iterrows():\n",
        "        article_content = row['content']\n",
        "        parser = PlaintextParser.from_string(article_content, Tokenizer(\"english\"))\n",
        "        summary = summarizer(parser.document, sentences_count=2)\n",
        "        summaries.append(str(summary))\n",
        "    return summaries\n",
        "\n",
        "excel_file_path = \"dataset.xlsx\"\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "num_cores = 4\n",
        "chunk_size = math.ceil(len(df) / num_cores)\n",
        "\n",
        "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "article_summaries = []\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
        "    chunk_summaries = executor.map(summarize_chunk, chunks)\n",
        "    for summaries in chunk_summaries:\n",
        "        article_summaries.extend(summaries)\n",
        "\n",
        "df['summary'] = article_summaries\n",
        "\n",
        "output_excel_file = \"summaries_sumy1.xlsx\"\n",
        "df.to_excel(output_excel_file, index=False)\n",
        "\n",
        "print(\"Summaries saved to\", output_excel_file)"
      ],
      "metadata": {
        "id": "_9S-M-q0Jvds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import math\n",
        "\n",
        "def generate_summary(content):\n",
        "    doc = nlp(content)\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "    summary = \" \".join(sentences[:2])\n",
        "    return summary\n",
        "\n",
        "def summarize_chunk(chunk):\n",
        "    summaries = []\n",
        "    for index, row in chunk.iterrows():\n",
        "        article_content = row['content']\n",
        "        summary = generate_summary(article_content)\n",
        "        summaries.append(summary)\n",
        "    return summaries\n",
        "\n",
        "excel_file_path = \"dataset.xlsx\"\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "\n",
        "num_cores = 4\n",
        "chunk_size = math.ceil(len(df) / num_cores)\n",
        "\n",
        "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "article_summaries = []\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
        "    chunk_summaries = executor.map(summarize_chunk, chunks)\n",
        "    for summaries in chunk_summaries:\n",
        "        article_summaries.extend(summaries)\n",
        "\n",
        "df['summary'] = article_summaries\n",
        "\n",
        "output_excel_file = \"summaries_spacy1.xlsx\"\n",
        "df.to_excel(output_excel_file, index=False)\n",
        "\n",
        "print(\"Summaries saved to\", output_excel_file)\n"
      ],
      "metadata": {
        "id": "qSZoCLYBGjaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "excel_file_path = 'summaries_spacy1.xlsx'\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "summary_column_name = 'summary'\n",
        "snippet_column_name = 'snippet'\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df[snippet_column_name].fillna(''))\n",
        "\n",
        "similarities = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    summary = row[summary_column_name]\n",
        "\n",
        "    try:\n",
        "        summary_vector = tfidf_vectorizer.transform([summary])\n",
        "        snippet_vector = tfidf_matrix[index]\n",
        "        similarity_score = cosine_similarity(summary_vector, snippet_vector)\n",
        "\n",
        "        similarity_percentage = (similarity_score[0][0] + 1) * 50\n",
        "        similarities.append(similarity_percentage)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating similarity for row {index + 1}: {e}\")\n",
        "        similarities.append(0)\n",
        "\n",
        "df['similarity'] = similarities\n",
        "\n",
        "updated_excel_file_path = 'Similarityspacy.xlsx'\n",
        "df.to_excel(updated_excel_file_path, index=False)\n",
        "print(\"Similarity percentages added and new Excel file created.\")\n"
      ],
      "metadata": {
        "id": "ZuXpheY8EIFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}